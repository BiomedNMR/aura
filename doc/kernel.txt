Kernel
======


OpenCL and Mobile
-----------------

There is a discussion about the viability of the OpenCL programming model
on Android. Google argues:

"
OpenCL uses the execution model first introduced in CUDA. In this model, 
a kernel is made up of one or many groups of workers, and each group has 
fast shared memory and synchronization primitives within that group. What 
this does is cause the description of an algorithm to be intermingled with 
how that algorithm should be scheduled on a particular architecture 
(because youâ€™re deciding the size of a group and when to synchronize within 
that group).
" 
from stackoverflow.com/questions/14385843#14389506

This is true and an interesting disadvantage of OpenCL/CUDA programming
models. How can we work around this? We can let the OpenCL driver decide 
the work group size. We can further specify minimum, maximum and required
sizes for a kernel to work (important for the algorithm and internal 
synchronization). Thus we can work around this problem.

kernel.set_min_mesh_elements()
kernel.set_max_mesh_elements()
kernel.set_min_bundle_elements()
kernel.set_max_bundle_elements()

This interface is not so nice, have to think about something better. maybe:

kernel.set_mesh_range(min, max);
kernel.set_bundle_range(min, max);

If a kernel can not execute on a device, how could we allow for a serial
implementation to execute?

Development for mobile should be observed very carefully. With such a huge
market force behind mobile development, mobile is bound to leap ahead of
HPC. While they have different requirements, they might still come up with
something ingenious.



Other
-----

A big difference betwen OpenCL and CUDA in terms of how kernels are
launched is:

in CUDA grid * block = number of threads running
in OpenCL grid = number of thread running
and block is a way of partitioning the number of threads

in CUDA, the .cu file must be compiled to cubin, fatbin or ptx.
to then load the ptx file in an application, you have to declare the
kernel to be extern "C", losing all template and namespace information.
CUDA probably does this internally in their compiler toolchain. Quite complex
what they built. I got the hint about this problem from 
http://stackoverflow.com/questions/11759972/



What is a good interface to launch kernels?

    CUDA has modules and functions, functions are part of modules, modules
can be a cubin file, a PTX file or a fatbin file.

the paremters are (driver API):
* kernel function pointer
* cache configuration: prefer none, prefer shared, prefer L1, prefer equal
* shared memory configuration: default, 4 byte bank size, 8 byte bank size
* grid-dim and block-dim (both 3 dimensional)
* amount of dynamic shared memory for each block
* kernel parameters
* stream the kernel should be launched in
* kernel that should be launched


    OpenCL has programs and kernels, kernels are part of programs, programs
contain 1...N kernels. Programs are built for devices and since there are
different devices, a program must be built multiple times if CPU and GPU
should be used.

the parameters are:
* kernel that should be launched
* stream the kernel should be launched in
* global work size and local work size
* list of events that need to occur befor kernel starts
* kernel arguments


    The interface of this mechanism is important. It would be really nice if 
we could register a kernel and then call it as a regular free function - this
would have to be done by the pre-processor though. Maybe not so nice.

What about:

feed.invoke(kernel, grid(10,10), block(20,40), shared(512), 
  arg1, arg2, arg3, arg4);

or

feed << invoke(kernel, grid(10,10), block(20,40), shared(512), 
  arg1, arg2, arg3, arg4);

or

invoke(kernel, grid(10,10), block(20,40), shared(512), 
  arg1, arg2, arg3, arg4, feed);

    If we use a member function, everything must be a member function so
the latter is probably best. Or the "<<" version where we could do something
like

feed << copy(A, B, size) 
     << invoke(kernel, grid(10,10), block(20,40), args(A, B)) 
     << copy(B, A, size); 

    I like the "args" parameter, this gives a nice structure to the call, we may
need to take care to avoid copying of parameters. Invoke should be callable 
with an optional feed argument though, to not enforce the above interface. I 
wonder if the "<<" version should be a higher level abstraction or if it should
be provided by the backend implementations.

The latter call could, in theory, implicitly build a double-buffered mechanism,
since kernel call and copy are coupled together. Something like:

feed << buffered_copy(A, B, size) 
     << invoke(kernel, grid(10,10), block(20,40), args(A, B)) 
     << buffered_copy(B, A, size); 

Hm, the latter is missing a lot to work, a loop construct for example, a way to
iterate over a buffer etc.


