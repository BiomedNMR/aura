Vector
======

It seems to be a fundamental difference if we want to either have a
library for linear algebra or a library with multi-dimensional vectors.
At least some people think so:
http://lists.boost.org/Archives/boost/2002/05/29151.php
But maybe this is wrong, I think we can do linear algebra (BLAS) and
multi-dimensional types.
It does then not make sense to have specific 1- 2- or 3- dimensional
types but only one type that is a multi-dimensional vector (including 
1- 2- and 3-dimensional).

so:

device d(0);
device_vector<T> v1(extent(d1, d2, d3, d4), d);

creates a 4 dimensional vector.

feed f(d);
device_vector<T> v2(extent(512, 512), d);
fft fh(d, f, v2.extent, fft::type::c2c);
fft_inverse(v2, v2, fh, f);

creates a 2 dimensional vector, an fft handle for that vector
and calculates the inverse fourier transform.
I like it.


We now have a buffer. Should the buffer be reference counted?
 * yes: probably easier to user
 * no: move semantics make multi-threading easier maybe?

Should a multi-dimensional densere array just be a view on a buffer
with an extent? C++ AMP has a very nice API that has similar features.
How would such a thing be passed to an API?

What should a multidimensional device vector be called?

device_nvector?
device_ndvector?
and
device_vector?
or simply
device_array?

it is not an std::vector if it can have multiple dimensions
it is also not an array for the same reasons.


What should a device vector be called?

dev_vector
device_vector
device_vec
memory
device_memory
dev_memory
memory_dev
device_array
device_table

Probably device_vector will be nice. 
The smart pointers library for example has shared_ptr and shared_array,
the standard now has shared_ptr, weak_ptr etc. This would favour 
device_vec but I still prefer device_vector.

Market research:

C++ AMP has:
array<T, rank> that copies data directly from a CPU buffer to GPU
array_view<T, rank> data is only copied if the view is used in 
a kernel function
index<rank> index point in array
extent<rank> a vector of rank values that specify the bounds of 
an ndim vector 

boost compute has
boost::compute::vector<T, allocator>

vexcl has
vexcl::vector

mtl4 has 
mtl::vector::dense_vector
that inherits from
::mtl::detail::contiguous_memory_block

paralution has
LocalVector<T>
that is used on a device after MoveToAccelerator is called.

Thrust has
thrust::device_vector

Martins recon toolkit does not have a vector nor a device ptr, it
has functions and pointers to memory. It uses backend functions to check
if a pointer lives on the device and dispatches at runtime device or host
functions

AGILE has
GPUVector and GPUMatrix

nt2 has
table<T>

codeare uses ViennaCL
ViennaCL has 
viennacl::vector 
(that can live on host and device)
and uses a shared ptr to handle memory behind the scenes, this
allows them to copy references to memory around that are still
valid even if the original vector goes out of scope

bolt has
device_vector<T>

cusp has 
array1d array2d and matrix

Memory Management
=================

How doe other projects handle memory? Do they have device pointer?

should it be a

device_ptr<T>, i.e. aura::backend::device_ptr<T>
remote_ptr<T>
ptr<T>, i.e. aura::backend::ptr<T>


Should it be reference counted? I think not, too much overhead.
It needs the actual pointer, a device/context (pointer to device)
and in the case of OpenCL it requires an offset.

I think this type is needed. It will allow type-safety from the
get-go since this type follows a device_malloc<T> of course.
