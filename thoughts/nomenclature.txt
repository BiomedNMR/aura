Nomenclature 
============

Names are clearly not sensible yet in Aura. Here are rules for naming
member functions for common tasks:

There is the question of having get_ style functions or without get
functions.

So I'm thinking get_* for Aura types and get_backend_* for accessing
backend types. This is already implemented.

The STL has

vector<T>:
	get_allocator (that speaks for get_ functions)
	data() -> T*
	begin(), end() -> iterator
	operator[] -> T
	front(), back() -> T

Now Aura has the device_ptr<T> in-between the container and T*.
The questions are:
	should there be a proper iterator or should device_ptr simply
	implement the iterator interface, possibly using the boost
	iterator helpers?

Maybe having a data() member to access the raw handle is a good
idea? Let's look at shared_ptr - it has the get() member.
To pass a raw-handle to a kernel one would have to call
A.begin().get() which is ugly
A.data() is cooler
but seems inconsistent.
Invoke could of course resolve the iterator to the raw handle but then
the kernel interface and the invoke call do not match which is confusing.
Then again, only on the iterator type is arithmetic possible, the OpenCL
backend would not allow A.data() + 15, instead it should be
std::advance(A.begin(), 15)



OpenCL has global_work_size, local_work_size, work-groups and work_items
CUDA has grids, blocks, threads

In OpenCL, the local_work_size subdivides the global_work_size
In CUDA. the grid consists of blocks

* jobs, local global, group
* threads, local global, group
* work, local, global, group
* block
* grid
* group
* domain
* cluster
* pack
* gang
* fiber
* lot
* assembly
* batch
* bundle
* fiber bundle
* packet
* lattice
* weave
* mesh
* netting
* net
* megafiber
* gigafiber
* lattice
* divisor
* denominator
* distribution
* fragmentation
* partition
* split
* sectioning
* layout
* fragmentation
* communition
* solvent
* fabric
* cells



fiber, fiber bundle, fiber bundle group
fiber, bundle, mesh <-
spark, flame, blaze
fiber, bundle, cluster
fiber, fiber_bundle, fiber_fabric
fiber, fiber_bundle, fibre_grid

for work-group/block:
synchronization domain 
single instruction domain (SIMD)

I prefer the "subdividing" nomanclature over the "consists of" way of
defining and partitioning work. It is a lot simpler. Or is it?

I think fiber, bundle and mesh are suitable metaphors for partitioning
work in the CUDA/OpenCL way.

fiber, bundle, cluster? 
fiber, fiber_bundle, fiber_cluster?
fiber, fiber_bundle, fiber_mesh?
stream, bundle, cluster?

Now should it be fiber_bundle, fiber_mesh

-----

Name of include file for kernel code:

* kernel_helper.hpp <--
* stdkernel.hpp
* stdkernellib.hpp
* devicelib.hpp
* libdevice.hpp

-----


OpenCL has command queues, CUDA has streams

I need a new word that encompasses both
The word should generalize on execution path (German: Ausfuehrungspfad),
or execution environment or execution envelope or something like this.
The word must be the name of the template argument for all library functions
that require an execution environment. Some options:

* fiber: this would be kind of a new concept but maybe it makes sense, since
it is not a thread but a concurrent environment
* execution path: to long
* task queue
* spark: a concept from haskell 
* transaction
* coroutine: a concept from concurrency research
* channel
* execution stream
* instruction stream
* command stream
* domain
* execution site
* execution context, exec_context, execution_context
* activity stream
* sequence
* isolate (from google dart)
* command set
* creek
* flood
* torrent
* pipeline 
* beam
* ray
* flurry
* breeze (this is nice and fits Aura)
* squall
* wave
* current (this is also nice)
* batch
* feed (wow, feed it nice)


The concept I am trying to name is in essence concurrency. It is an independent
stream of instructions. The programmer explicitly marks theses instructions
as independent. If the hardware is capable, these instructions can be executed
in parallel.

The concept encompasses both a location of execution (accelerator) but also a 
logical group of execution (concurrency).

I think execution context might be the correct phrase, it can be either a 
device or a stream.

Actually no. There should be just 2 concepts: a device (execution site) and 
an execution stream allowing some form of concurrency. The question now is, 
how should they be interlinked and do they have to be interlinked. If they
should be interlinked, this will certainly have to be done with thread-safe
data structures that could degrade performance and increase complexity at
an already very low level. So maybe this can be avoided. Once concsequence is
that we can not have a device_synchronize(device) functin because neither 
OpenCL nor CUDA actually provide that. If we wanted to do that we would need
to store all streams and and command queues somewhere. So we only provide
stream synchronization.

Let's call it breeze. Well, breeze fits the Aura metaphor nicely but I
think feed is a more accurate word for it. A device feed. Nice. 



The type representing a file containing kernel code (fatbin, ptx, cl-source)
requires a name.

kernel_code_file
kernel_file
module_file
backend_file

